{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1a140a11-bc70-4d42-bf0e-32001f98b864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a12d252-5184-4ce2-8793-e1383b2fb565",
   "metadata": {},
   "source": [
    "## 1) Prepare URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0774111d-bb8e-4e5a-af1c-9c8b351cfbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(position, location):\n",
    "    \"\"\"Generate url from position and location\"\"\"\n",
    "    template = 'https://www.indeed.com/jobs?q={}&l={}'\n",
    "    position = position.replace(' ', '+')\n",
    "    location = location.replace(' ', '+')\n",
    "    url = template.format(position, location)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abba4e6b-5ba5-4571-b363-98aef76bfb21",
   "metadata": {},
   "source": [
    "### 1a) Get Clinical Informatics Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "33bf1406-2bd2-4df0-b0aa-ffa6a1fc0a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.indeed.com/jobs?q=Clinical+Informatics&l=USA\n"
     ]
    }
   ],
   "source": [
    "url = get_url('Clinical Informatics', 'USA')\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01db5f70-9f50-418f-bb8c-026d40af4e67",
   "metadata": {},
   "source": [
    "### 1b) Get Health Informatics Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1a79e83c-e935-42a3-91ab-241fc700fb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.indeed.com/jobs?q=Health+Informatics&l=USA\n"
     ]
    }
   ],
   "source": [
    "url = get_url('Health Informatics', 'USA')\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934c4c18-f832-458c-ba7f-eea7bdc3056f",
   "metadata": {},
   "source": [
    "## 2) Search Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fd8956c6-7a2d-47d4-b141-65208a8ca052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "#print(response)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#print(soup)#job_seen_beacon\n",
    "cards = soup.find_all('div', 'job_seen_beacon')\n",
    "#print(cards)\n",
    "print(len(cards))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0521d844-2665-4105-abcf-e755a71a33d1",
   "metadata": {},
   "source": [
    "## 3) Get job post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51299e94-b113-4fb3-9000-9aeab9f4c160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_record(card):\n",
    "    \"\"\"Extract job data from a single record\"\"\"\n",
    "    job_header= card.table.tbody.tr.td\n",
    "    job_title = job_header.div.h2.a.span.get('title')\n",
    "    job_link= 'https://www.indeed.com'+job_header.div.h2.a.get('href')\n",
    "\n",
    "    try:\n",
    "      job_company=job_header.find('span','companyName').text.strip()\n",
    "    except:\n",
    "      print(\"No company name found for job \"+job_title)\n",
    "\n",
    "    try:\n",
    "      job_ratings=job_header.find('span','ratingsDisplay').text.strip()\n",
    "    except:\n",
    "      print(\"No job ratings found for job \"+job_title)\n",
    "      job_ratings=\"\"\n",
    "\n",
    "    try:\n",
    "      job_location=job_header.find('div','companyLocation').text.strip()\n",
    "    except:\n",
    "      print(\"No job location found for job \"+job_title)\n",
    "\n",
    "    try:\n",
    "      job_date=card.find('span','date').text.strip().replace(\"Posted\", \"\").replace(\"EmployerActive\", \"\").strip()\n",
    "      print(job_date)\n",
    "    except:\n",
    "      print(\"No job date found for job \"+job_title)\n",
    "\n",
    "    try:\n",
    "      job_snippets=card.find_all('div','attribute_snippet')\n",
    "      print(len(job_snippets))\n",
    "    except:\n",
    "      #print(\"No job location found for job \"+job_title)\n",
    "      job_snippets=\"\"\n",
    "\n",
    "    if job_snippets!=\"\":\n",
    "        try:      \n",
    "          job_salary=job_snippets.find('svg',{'aria-label':'Salary'})\n",
    "        except:\n",
    "          #print(\"No job salary found for job \"+job_title)\n",
    "          job_salary=\"\"\n",
    "        try:      \n",
    "          job_type=job_snippets.find('svg',{'aria-label':'Job type'})\n",
    "        except:\n",
    "          #print(\"No job type found for job \"+job_title)\n",
    "          job_type=\"\"\n",
    "        try:      \n",
    "          job_shift=job_snippets.find('svg',{'aria-label':'Shift'})\n",
    "        except:\n",
    "          #print(\"No job shift found for job \"+job_title)\n",
    "          job_shift=\"\"\n",
    "    else:\n",
    "        job_salary=\"\"\n",
    "        job_type=\"\"\n",
    "        job_shift=\"\"\n",
    "\n",
    "    try:      \n",
    "        job_brief=card.find('div','job-snippet').ul.li.text.strip().replace('\\n', ' ')\n",
    "    except:\n",
    "        job_brief=\"\"\n",
    "    \n",
    "    print(job_brief)\n",
    "\n",
    "    collect_date = datetime.today().strftime('%Y-%m-%d')     \n",
    "    record = (job_title, job_link, job_company, job_location, job_date, collect_date, job_brief, job_ratings,job_salary, job_type, job_shift)\n",
    "    return record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87087894-c1d5-467d-940b-36e291f76cff",
   "metadata": {},
   "source": [
    "## 4) Get all jobs by scraping the next pages of the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b6d46-1f86-46ee-a773-4612eb51fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "records=[]\n",
    "url=get_url('Health Informatics', 'USA') \n",
    "#url=get_url('Clinical Informatics', 'USA') # Uncomment and comment the above line to retrieve the Health Informatics Jobs\n",
    "for i in range(67):\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    cards = soup.find_all('div', 'job_seen_beacon')\n",
    "\n",
    "    for card in cards:\n",
    "        record = get_record(card)\n",
    "        if record in records:\n",
    "            continue\n",
    "        else:\n",
    "            records.append(record)\n",
    "    try: \n",
    "        url = 'https://www.indeed.com' + soup.find('a', {'aria-label': 'Next'}).get('href')\n",
    "    except AttributeError:\n",
    "        print('search results error in iter '+str(i))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2497dbd3-3f58-4b85-8300-5b0180648ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "831\n"
     ]
    }
   ],
   "source": [
    "print(len(records))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ca5647-0d59-474b-9f5e-ea250c053e23",
   "metadata": {},
   "source": [
    "## 5) Save job posts in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f921eea7-80bb-41a3-be24-f5f203b5e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the job data\n",
    "with open('results.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['JobTitle', 'JobUrl','Company', 'Location', 'PostDate', 'ExtractDate', 'Summary','Job_ratings', 'Salary', 'JobType','JobShift'])\n",
    "    writer.writerows(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4c9c04-ff74-40f5-a561-bb66866afeb8",
   "metadata": {},
   "source": [
    "## 6) Getting jobs briefing: title, URL, summary, location, etc from the search result of the previous steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "461a9f24-cee1-4b13-b567-5269d1893000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "jobs = pd.read_csv('results.csv')\n",
    "titles = jobs['JobTitle'].tolist()\n",
    "urls = jobs['JobUrl'].tolist()\n",
    "company = jobs['Company'].tolist()\n",
    "location= jobs['Company'].tolist()\n",
    "PostDate= jobs['PostDate'].tolist()\n",
    "ExtractDate= jobs['ExtractDate'].tolist()\n",
    "Summary= jobs['Summary'].tolist()\n",
    "Job_ratings= jobs['Job_ratings'].tolist()\n",
    "Salary= jobs['Salary'].tolist()\n",
    "JobType= jobs['JobType'].tolist()\n",
    "JobShift= jobs['JobShift'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59228463-c9cf-428d-aea5-9c2d1169f611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-3.7.0-py2.py3-none-any.whl (25 kB)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-0.20.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: requests in /Users/aelnosho/opt/anaconda3/lib/python3.9/site-packages (from webdriver-manager) (2.26.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aelnosho/opt/anaconda3/lib/python3.9/site-packages (from requests->webdriver-manager) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aelnosho/opt/anaconda3/lib/python3.9/site-packages (from requests->webdriver-manager) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/aelnosho/opt/anaconda3/lib/python3.9/site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/aelnosho/opt/anaconda3/lib/python3.9/site-packages (from requests->webdriver-manager) (1.26.7)\n",
      "Installing collected packages: python-dotenv, webdriver-manager\n",
      "Successfully installed python-dotenv-0.20.0 webdriver-manager-3.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installing the web driver for web browser\n",
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f968284-152b-41a6-b10f-17c85086b4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 101.0.4951\n",
      "[WDM] - Get LATEST chromedriver version for 101.0.4951 google-chrome\n",
      "[WDM] - There is no [mac64] chromedriver for browser 101.0.4951 in cache\n",
      "[WDM] - About to download new driver from https://chromedriver.storage.googleapis.com/101.0.4951.41/chromedriver_mac64.zip\n",
      "[WDM] - Driver has been saved in cache [/Users/aelnosho/.wdm/drivers/chromedriver/mac64/101.0.4951.41]\n",
      "/var/folders/7v/g_t4898131v7f09flvsqg0v80000gn/T/ipykernel_1115/2160715986.py:4: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "43a70c38-571c-4748-853e-7393fb0f725a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# The below is the path of the web browser driver on the machine used for this research.\n",
    "# To reuse, change the below file location to the location of the web browser driver on your local machine\n",
    "os.path.exists('/Users/aelnosho/.wdm/drivers/chromedriver/mac64/101.0.4951.41')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b243d-d95e-4582-a996-7475d6e73758",
   "metadata": {},
   "source": [
    "## 7) Getting jobs details by opening each job URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9de5fc-76ef-4f80-803a-22bd4ed1b7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "count=0\n",
    "jobsdesc=[]\n",
    "for l in urls:\n",
    "    url = l\n",
    "    # initiating the webdriver. Parameter includes the path of the webdriver.\n",
    "    driver = webdriver.Chrome('/Users/aelnosho/.wdm/drivers/chromedriver/mac64/101.0.4951.41/chromedriver') \n",
    "    try:\n",
    "        driver.get(url) \n",
    "\n",
    "        # this is just to ensure that the page is loaded\n",
    "        time.sleep(5) \n",
    "\n",
    "        html = driver.page_source\n",
    "        # this renders the JS code and stores all\n",
    "        # of the information in static HTML code.\n",
    "\n",
    "        # Now, we could simply apply bs4 to html variable\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        try:\n",
    "            salary = soup.find('div', {'id' : 'salaryGuide'}).ul.text\n",
    "            Salary[count]=salary\n",
    "        except:\n",
    "            print(\"No salary found for job \")\n",
    "        jobDescription = soup.find('div', {'id' : 'jobDescriptionText'}).text\n",
    "        #print(jobDescription)\n",
    "        jobsdesc.append(jobDescription)\n",
    "    except:\n",
    "        print(\"Description of job number \"+str(count)+\" was not retrieved\")\n",
    "        jobsdesc.append(\"\")\n",
    "    count=count+1\n",
    "    \n",
    "driver.close() # closing the webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9115db26-e160-4b2d-a241-3c890ef2d89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "571\n"
     ]
    }
   ],
   "source": [
    "print(len(jobsdesc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f330772e-b56f-4613-9e67-2ec24942fe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "831\n"
     ]
    }
   ],
   "source": [
    "## Checking for error loading the job descriptions pages detail\n",
    "diff=len(urls)-count\n",
    "print(diff)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e266f-95b9-497f-8069-d99214b7f352",
   "metadata": {},
   "source": [
    "## 8) load remaining jobs that did not load in previous point due to error loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3508fe-551c-4af1-965b-7cab717b51a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(count, len(jobsdesc)):\n",
    "    url=urls[u]\n",
    "    print(url)\n",
    "    driver = webdriver.Chrome('/Users/aelnosho/.wdm/drivers/chromedriver/mac64/101.0.4951.41/chromedriver') \n",
    "    driver.get(url) \n",
    "\n",
    "    # this is just to ensure that the page is loaded\n",
    "    time.sleep(5) \n",
    "\n",
    "    html = driver.page_source\n",
    "    # this renders the JS code and stores all\n",
    "    # of the information in static HTML code.\n",
    "\n",
    "    # Now, we could simply apply bs4 to html variable\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    try:\n",
    "        salary = soup.find('div', {'id' : 'salaryGuide'}).ul.text\n",
    "        Salary[count]=salary\n",
    "    except:\n",
    "        print(\"No salary found for job \")\n",
    "        \n",
    "    jobDescription = soup.find('div', {'id' : 'jobDescriptionText'}).text\n",
    "    jobsdesc.append(jobDescription)\n",
    "    count=count+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fb0179-d219-44fa-b278-9ddfebb8ec53",
   "metadata": {},
   "source": [
    "## 9) save jobs with job description into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea3a2afb-f9c3-4322-9862-b08b19680369",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated = pd.DataFrame({'Job_titles':titles,'Job_URLS':urls,'Company':company,'location':location,'PostDate':PostDate,'ExtractDate':ExtractDate,'Summary':Summary,'Job_ratings':Job_ratings,'Job_description':jobsdesc,'Salary':Salary,'JobType':JobType,'JobShift':JobShift})\n",
    "df_updated.to_csv('JobsDetails.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1a5ec5-92bf-45b8-81a6-d1e48ff29e39",
   "metadata": {},
   "source": [
    "## 10) Analyze jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e926cf-8927-4517-8502-b78f8f82712a",
   "metadata": {},
   "source": [
    "### 10A) Clean the job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "32135cd7-2065-4139-a269-891806c44979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_updated = pd.read_csv(r'JobDetailsAnalysis-HI.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b2475882-e060-4fd2-b6c3-af7b59758786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def cleanURLEmail(strip):\n",
    "    # extract emails\n",
    "    match = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', strip)\n",
    "    temp = strip\n",
    "    for m in match:\n",
    "        temp = strip.replace(m, '', 100)\n",
    "        strip = temp\n",
    "    match3 = re.findall(r'^https?:\\/\\/.*[\\r\\n]*', strip)\n",
    "    for m in match3:\n",
    "        temp = strip.replace(m, '', 100)\n",
    "        strip = temp\n",
    "    return strip\n",
    "\n",
    "\n",
    "#Remove Stop words, punctuation, and extract lemmatization, and stemming.\n",
    "def clean(doc,stop,exclude):\n",
    "    lemma = WordNetLemmatizer()\n",
    "    stop_free = \" \".join([i for i in str(doc).lower().split() if i not in stop])\n",
    "    no_punc = map(lambda x: re.sub(\"[^a-z]\", \" \", x), stop_free)\n",
    "    punc_free = \"\".join(ch for ch in no_punc if ch not in exclude)\n",
    "    normalized=\"\"\n",
    "    stemmed = \"\"\n",
    "    for word in punc_free.split():\n",
    "        try:\n",
    "            lWord=lemma.lemmatize(word)\n",
    "        except:\n",
    "            continue\n",
    "        normalized=normalized+lWord+\" \"\n",
    "    stop_free2 = \" \".join([i for i in normalized.lower().split() if i not in stop])\n",
    "    return stop_free2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e3d6d3e3-b225-4ce1-ad13-b531c91af3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7v/g_t4898131v7f09flvsqg0v80000gn/T/ipykernel_812/3880222035.py:17: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('/Users/aelnosho/.wdm/drivers/chromedriver/mac64/101.0.4951.41/chromedriver')\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "#import simplejson\n",
    "from nltk import ngrams\n",
    "import datetime\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import requests\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome('/Users/aelnosho/.wdm/drivers/chromedriver/mac64/101.0.4951.41/chromedriver') \n",
    "us_state_abbrev = {\n",
    "    'Alabama': 'AL',\n",
    "    'Alaska': 'AK',\n",
    "    'Arizona': 'AZ',\n",
    "    'Arkansas': 'AR',\n",
    "    'California': 'CA',\n",
    "    'Colorado': 'CO',\n",
    "    'Connecticut': 'CT',\n",
    "    'Delaware': 'DE',\n",
    "    'Florida': 'FL',\n",
    "    'Georgia': 'GA',\n",
    "    'Hawaii': 'HI',\n",
    "    'Idaho': 'ID',\n",
    "    'Illinois': 'IL',\n",
    "    'Indiana': 'IN',\n",
    "    'Iowa': 'IA',\n",
    "    'Kansas': 'KS',\n",
    "    'Kentucky': 'KY',\n",
    "    'Louisiana': 'LA',\n",
    "    'Maine': 'ME',\n",
    "    'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI',\n",
    "    'Minnesota': 'MN',\n",
    "    'Mississippi': 'MS',\n",
    "    'Missouri': 'MO',\n",
    "    'Montana': 'MT',\n",
    "    'Nebraska': 'NE',\n",
    "    'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH',\n",
    "    'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM',\n",
    "    'New York': 'NY',\n",
    "    'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND',\n",
    "    'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR',\n",
    "    'Pennsylvania': 'PA',\n",
    "    'Rhode Island': 'RI',\n",
    "    'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD',\n",
    "    'Tennessee': 'TN',\n",
    "    'Texas': 'TX',\n",
    "    'Utah': 'UT',\n",
    "    'Vermont': 'VT',\n",
    "    'Virginia': 'VA',\n",
    "    'Washington': 'WA',\n",
    "    'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI',\n",
    "    'Wyoming': 'WY',\n",
    "}\n",
    "statesBSC={}\n",
    "statesMaster={}\n",
    "statesCertifications={}\n",
    "states={}\n",
    "for k, v in us_state_abbrev.items():\n",
    "    statesBSC[v]=0\n",
    "    statesMaster[v] = 0\n",
    "    statesCertifications[v] = 0\n",
    "    states[v]=0\n",
    "\n",
    "\n",
    "def cleanJobDescription(description):\n",
    "    doc_complete=[]\n",
    "    doc_clean=[]\n",
    "    tList=[]\n",
    "    exclude = set(string.punctuation)\n",
    "    stop = set(stopwords.words('english'))\n",
    "    lines = open(\"stop3\").read().splitlines()\n",
    "    for word in lines:\n",
    "        #print(word)\n",
    "        stop.add(word)\n",
    "    myStop = [\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\",\n",
    "                 \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\",\n",
    "                 \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\",\n",
    "                 \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\",\n",
    "                 \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\",\n",
    "                 \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\",\n",
    "                 \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\",\n",
    "                 \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\",\n",
    "                 \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\",\n",
    "                 \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\",\n",
    "                 \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\",\n",
    "                 \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\",\n",
    "                 \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "                 \"interest\", \"into\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\",\n",
    "                 \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\",\n",
    "                 \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "                 \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\",\n",
    "                 \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\",\n",
    "                 \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "                 \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\",\n",
    "                 \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\",\n",
    "                 \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\",\n",
    "                 \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\",\n",
    "                 \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\",\n",
    "                 \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\",\n",
    "                 \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\",\n",
    "                 \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "                 \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\",\n",
    "                 \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\",\n",
    "                 \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "                 \"yourselves\", \"the\", 'http', 'https', '://', 'www', 'com', '8800', '...', '....','//','/','..', 'yep', '.).', '](#', '.:).',\n",
    "                '++..', 'github', 'etc', 'also', 'org', 'gee', 'let', 'know', 'ever',\n",
    "                'vcntr', 'falseamount', 'isig']\n",
    "    for word in myStop:\n",
    "        stop.add(word)\n",
    "        \n",
    "    #strip HTML tags from tweet\n",
    "    temp=\"\"\n",
    "    #strip = strip_tags(description)\n",
    "    #strip1 = cleanURLEmail(description)\n",
    "    strip1 = str(description).replace(\".\",\"\")\n",
    "    strip2 = clean(strip1,stop,'')\n",
    "    return(strip2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ccd3745c-8b4a-4dba-a823-c942f140f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated['cleanDesc']= df_updated.apply(lambda x : cleanJobDescription(x['Job_description']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69ac358-8cfd-48b6-9f43-fd197e8ec88a",
   "metadata": {},
   "source": [
    "### 10b) Get skills, certificates, training, and degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "37c53a47-8955-436f-b7e0-674b3f902c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcation to flag the job if it required or prefer a professional certification or a university degree\n",
    "def checkDegrees(description,degreeCheck):\n",
    "    result=0\n",
    "    graduateKeywords=['mba','msc', 'md', 'master', 'phd', 'doctorate']\n",
    "    bachelorKeywords = ['college', 'bachelor', 'bsc']\n",
    "    degrees=['mba','msc', 'md', 'master', 'phd', 'doctorate','college', 'bachelor', 'bsc']\n",
    "    if degreeCheck in str(description):\n",
    "        result= 1\n",
    "    elif degreeCheck == 'bachelor':\n",
    "        for word in bachelorKeywords:\n",
    "            if word in str(description):\n",
    "                result = 1\n",
    "                break\n",
    "    elif degreeCheck == 'graduate':\n",
    "        for word in graduateKeywords:\n",
    "            if word in str(description):\n",
    "                result = 1\n",
    "                break\n",
    "    elif degreeCheck == 'certifications':\n",
    "        if \"ertified\" in str(description) or \"ertificat\" in str(description):\n",
    "                result = 1\n",
    "    elif degreeCheck == 'otherDegree':\n",
    "        count=0\n",
    "        if 'degree' in str(description):\n",
    "            for word in degrees:\n",
    "                if word in str(description):\n",
    "                    result = 0\n",
    "                    break\n",
    "                count=count+1\n",
    "                if count>8: \n",
    "                    result = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "07e5e1fe-f0bb-4dee-a9e0-dd802d94472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that extracts the certifications, skills, programming languages, data visualization tools required \n",
    "# or prefered in the job description\n",
    "# check to exclude certification if has any of the following degrees=['mba','msc', 'md', 'master', 'phd', 'doctorate','college', 'bachelor', 'bsc']\n",
    "def getText(description, search):\n",
    "    count = 0\n",
    "    degree=0\n",
    "    # For professional certifications exclude, any academic certification \n",
    "    degrees=['mba','msc', 'md', 'master', 'phd', 'doctorate','college', 'bachelor', 'bsc']\n",
    "    try:\n",
    "        TexttotSet=set()\n",
    "        words=description.lower().split()\n",
    "        count=0\n",
    "        cert=0\n",
    "        for w in words:\n",
    "            temp=\"\"\n",
    "            if (search==\"certifications\" and (\"ertified\" in w or \"ertificat\" in w)) or (search==\"skills\" and \"skill\" in w) or (search==\"trainings\" and \"train\" in w)or (search==\"degrees\" and \"degree\" in w) or (search==\"license\" and \"license\" in w) or (search==\"visualization\" and \"visualization\" in w) or (search==\"programming\" and \"progamming\" in w):\n",
    "                degree=0\n",
    "                counter=4\n",
    "                i=count-1\n",
    "                tempQueue=[]\n",
    "                while(i>-1 and counter>0):\n",
    "                    if words[i] in degrees:\n",
    "                        degree=1\n",
    "                        break\n",
    "                    tempQueue.append(words[i])\n",
    "                    counter-=1\n",
    "                    i-=1\n",
    "                if degree==0:\n",
    "                    while(len(tempQueue)>0):\n",
    "                        temp=tempQueue.pop(0)+\" \"+temp\n",
    "                counter=1\n",
    "                while (degree==0 and counter<5 and counter+count<len(words)):\n",
    "                    if words[count+counter] in degrees:\n",
    "                        degree=1\n",
    "                        break\n",
    "                    temp=temp+\" \"+words[count+counter]\n",
    "                    counter+=1\n",
    "                if degree==0:\n",
    "                    TexttotSet.add(temp)\n",
    "            count+=1\n",
    "        \n",
    "        # Extract the certification or skill name that could consists of one word, two words, three words...upto six words\n",
    "        s=\"\"\n",
    "        for t in TexttotSet:\n",
    "            Text=str(t)\n",
    "            #Bigrams\n",
    "            bigramsText=\"\"\n",
    "            n = 2\n",
    "            n_grams = ngrams(Text.split(), n)\n",
    "            for grams in n_grams:\n",
    "                temp=\"\"\n",
    "                temp=grams[0]+\"_\"+grams[1]\n",
    "                bigramsText=bigramsText+\" \"+temp\n",
    "\n",
    "            #trigrams\n",
    "            trigramsText = \"\"\n",
    "            n = 3\n",
    "            n_grams = ngrams(Text.split(), n)\n",
    "            for grams in n_grams:\n",
    "                temp=grams[0]+\"_\"+grams[1]+\"_\"+grams[2]\n",
    "                trigramsText = trigramsText + \" \" + temp\n",
    "\n",
    "            fourgramsText = \"\"\n",
    "            n = 4\n",
    "            n_grams = ngrams(Text.split(), n)\n",
    "            for grams in n_grams:\n",
    "                temp=\"\"\n",
    "                temp=grams[0]+\"_\"+grams[1]+\"_\"+grams[2]+\"_\"+grams[3]\n",
    "                fourgramsText = fourgramsText + \" \" + temp\n",
    "\n",
    "            fivegramsText=\"\"\n",
    "            n = 5\n",
    "            n_grams = ngrams(Text.split(), n)\n",
    "            for grams in n_grams:\n",
    "                temp=\"\"\n",
    "                temp=grams[0]+\"_\"+grams[1]+\"_\"+grams[2]+\"_\"+grams[3]+\"_\"+grams[4]\n",
    "                fivegramsText = fivegramsText + \" \" + temp\n",
    "\n",
    "            sixgramsText=\"\"\n",
    "            n = 6\n",
    "            n_grams = ngrams(Text.split(), n)\n",
    "            for grams in n_grams:\n",
    "                temp=\"\"\n",
    "                temp=grams[0]+\"_\"+grams[1]+\"_\"+grams[2]+\"_\"+grams[3]+\"_\"+grams[4]+\"_\"+grams[5]\n",
    "                sixgramsText = sixgramsText + \" \" + temp\n",
    "\n",
    "            s=s+Text+\" \"+bigramsText+\" \"+trigramsText+\" \"+fourgramsText+\" \"+fivegramsText+\" \"+sixgramsText+\" \"\n",
    "            return str(s.strip())\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c1afa8d4-9511-4ef0-bf02-799ebc94db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated['certifications']= df_updated.apply(lambda x : getText(x['cleanDesc'],'certifications'),axis=1)\n",
    "df_updated['skills']= df_updated.apply(lambda x : getText(x['cleanDesc'],'skills'),axis=1)\n",
    "df_updated['trainings']= df_updated.apply(lambda x : getText(x['cleanDesc'],'trainings'),axis=1)\n",
    "df_updated['degrees']= df_updated.apply(lambda x : getText(x['cleanDesc'],'degrees'),axis=1)\n",
    "df_updated['license']= df_updated.apply(lambda x : getText(x['cleanDesc'],'license'),axis=1)\n",
    "df_updated['programming']= df_updated.apply(lambda x : getText(x['cleanDesc'],'programming'),axis=1)\n",
    "df_updated['visualization']= df_updated.apply(lambda x : getText(x['cleanDesc'],'visualization'),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9e4b23e6-1079-464c-82a1-5e1a1de5e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated['bachelor']=df_updated.apply(lambda x : checkDegrees(x['Job_description'],'bachelor'),axis=1)\n",
    "df_updated['graduate']=df_updated.apply(lambda x : checkDegrees(x['Job_description'],'graduate'),axis=1)\n",
    "df_updated['otherDegree']=df_updated.apply(lambda x : checkDegrees(x['Job_description'],'otherDegree'),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "72d20e30-e951-48b3-b799-1d74ffbd6b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkCollege(bachelor,graduate):\n",
    "    if bachelor ==1 or graduate==1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "df_updated['college']=df_updated.apply(lambda x : checkCollege(x['bachelor'],x['graduate']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5887b9c-3373-4a78-895b-9eed43006e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jobs with undergraduate or graduate degrees requested\n",
    "df_updated[df_updated['college']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0836eb09-0c18-4061-9a4a-e70e14784a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jobs with College graduate degrees requested\n",
    "df_updated[df_updated['graduate']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559465d7-3a69-44ed-af63-68cd3068058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jobs that requests bachelors degrees\n",
    "df_updated[df_updated['bachelor']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "01959e49-91b9-414d-a4ab-4615b73c393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag the job if a certification is requested\n",
    "def checkCertificationLenght(certification):\n",
    "    if certification is None:\n",
    "        return 0\n",
    "    return len(certification)\n",
    "df_updated['certificationFlag']=df_updated.apply(lambda x : checkCertificationLenght(x['certifications']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "83bab47f-6d0e-40af-899f-3812db33d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated[df_updated['certificationFlag']>0].to_csv('certifications.csv')\n",
    "\n",
    "# The reason why 40% of the jobs listed prefering or requiring certification while \n",
    "# the jobs with specifically mentioned certifications are almost 10% to 15% is that\n",
    "# most of the jobs either mentioned that certification is required or prefered without specifying like:\n",
    "# \"Certification relevant to training,\" \"Industry designation certifications,\" \"Industry-focused Certifications,\"\n",
    "# or \"Speciality certification is required\", etc. Also, some jobs mentioned \"Certification support\" as part \n",
    "# of the professional development benefits in the job to attract candidates focusing on continous improvement\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e112584-1a55-46c8-b1c5-0cb96b956fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the location of the jobs by state\n",
    "\n",
    "# Checking jobs that have the flexibility of working remotely\n",
    "def checkRemote(location):\n",
    "    if 'remote' in location.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "df_updated['remote']=df_updated.apply(lambda x : checkRemote(x['location']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "056e1bc9-126c-418f-a31b-a471e9da0bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "statesAbbrev=list(us_state_abbrev.values())\n",
    "statesAb=[]\n",
    "for st in statesAbbrev:\n",
    "    statesAb.append(\" \"+st)\n",
    "states=list(us_state_abbrev.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "975a7da5-0c1f-4f6c-8ae8-a32f4b8e917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkState(location):\n",
    "    for st in statesAb:\n",
    "        if st in location:\n",
    "            return st.strip()\n",
    "    for st in states:\n",
    "        if st in location:\n",
    "            return us_state_abbrev[st]\n",
    "    return '0'\n",
    "df_updated['states']=df_updated.apply(lambda x : checkState(x['location']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d0c3c8b-6855-49b1-ab97-66ba5d3b8b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated['states'].value_counts().to_csv('States.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d930a362-00ce-495f-b798-246c7c531e92",
   "metadata": {
    "tags": []
   },
   "source": [
    "### N.B. 1/4 of the jobs have a remote working flexibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fcd2e5-27f1-433f-a031-7eb33cee5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated[df_updated['remote']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb0c6c1-b0e1-4fa7-ba57-cc0c1c27f77e",
   "metadata": {},
   "source": [
    "### Checking job flexibility with college degree requirement (no correlation found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d9795655-cb44-4bfe-8942-68cc21887c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.07909216195943626"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_updated['remote'].corr(df_updated['college'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f5c28e-58d7-4b02-9a44-3e88ebf68aeb",
   "metadata": {},
   "source": [
    "### Checking the skills, certifications, data visualization tools in highest demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3243aec0-1cac-46f8-98a1-a197c0703b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHighDemandJobDescription(df,demand):\n",
    "    def preprocess(text):\n",
    "        tokens = []\n",
    "        tokenizer = nltk.tokenize.RegexpTokenizer(r\"[a-z]+\")\n",
    "        for token in tokenizer.tokenize(str(text)):\n",
    "            tokens.append(token)\n",
    "        return tokens\n",
    "    if demand=='skills':\n",
    "        List=df['skills'].tolist()\n",
    "    elif demand=='certifications':\n",
    "        List=df['certifications'].tolist()\n",
    "    elif demand=='trainings':\n",
    "        List=df['trainings'].tolist()\n",
    "    elif demand=='license':\n",
    "        List=df['license'].tolist()\n",
    "    elif demand=='programming':\n",
    "        List=df['programming'].tolist()\n",
    "    elif demand=='visualization':\n",
    "        List=df['visualization'].tolist()\n",
    "    else:\n",
    "        List=df['degrees'].tolist()\n",
    "    processed = list(map(preprocess, List))\n",
    "    fdist = nltk.FreqDist([token for doc in processed for token in doc])\n",
    "    print(fdist.tabulate(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6249197-9c4f-4080-8850-8244d620b105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get high demand skills\n",
    "getHighDemandJobDescription(df_updated,'skills')\n",
    "# Get high demand certifications\n",
    "getHighDemandJobDescription(df_updated,'certifications')\n",
    "# Top university degrees listed \n",
    "getHighDemandJobDescription(df_updated,'degrees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce472a3c-4e5a-48c8-88ce-c5579bd18efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get high demand data visualization and programming tools\n",
    "import numpy as np\n",
    "df=df_updated\n",
    "df['visualization'].replace('', np.nan).dropna(inplace=True)\n",
    "print(df)\n",
    "getHighDemandJobDescription(df,'visualization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce5f42d-19a2-4588-952b-7e4ec8408d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "maskSkill1 = df_updated['cleanDesc'].str.contains(r'SQL', na=True)\n",
    "df_updated[maskSkill1]\n",
    "\n",
    "# The number of jobs requesting each skill:\n",
    "# Analytics and Problem Sovling 623\n",
    "# Communication skills 337\n",
    "# Oral Communication 238\n",
    "# Project Management 199\n",
    "# Statistics 198\n",
    "# Interpersonal skills 172\n",
    "# Critical thinking 67\n",
    "\n",
    "# The top mentioned tools\n",
    "# Excel 514\n",
    "# SQL 150\n",
    "\n",
    "# Tool listed tools\n",
    "\n",
    "#mask6 = df_updated['cleanDesc'].str.contains(r'js', na=True) & df_updated['cleanDesc'].str.contains(r'certificat', na=True)\n",
    "#mask6 = df_updated['cleanDesc'].str.contains(r'critical thinking', na=True) & df_updated['graduate']==1\n",
    "#mask6 = df_updated['cleanDesc'].str.contains(r'java', na=True)\n",
    "#mask6 = df_updated['Job_description'].str.contains(r' R ', na=True)\n",
    "#mask6 = df_updated['cleanDesc'].str.contains(r'power bi', na=True)\n",
    "df_updated[mask6]\n",
    "# Python 74\n",
    "# Tableau 73\n",
    "# Java 29\n",
    "# R 24\n",
    "# SPSS 14\n",
    "# Power BI 13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "78536174-da05-42d0-9916-abf6fcc280ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the top listed keywords for certifications in spreadsheet\n",
    "CertificationsWording_list = df_updated[\"certifications\"].tolist()\n",
    "certificationDictionary={}\n",
    "for certificationWording in CertificationsWording_list:\n",
    "    if certificationWording is None:\n",
    "        continue\n",
    "    l= certificationWording.split()\n",
    "    for w in l:\n",
    "        if w in certificationDictionary:\n",
    "            certificationDictionary[w]+=1\n",
    "        else:\n",
    "            certificationDictionary[w]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "bcb68057-c192-4b79-832a-53a042eb18be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('certificatesKeywords.csv','w') as file:\n",
    "    line=\"word,count\"\n",
    "    file.write(line)\n",
    "    file.write('\\n')\n",
    "    for k in certificationDictionary:\n",
    "        line=k+\",\"+str(certificationDictionary[k])\n",
    "        file.write(line)\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d02a74-028a-4acd-a952-ce6993c7865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking in how many jobs certain certifications was listed\n",
    "mask3 = df_updated['cleanDesc'].str.contains(r'cpc', na=True)\n",
    "df_updated[mask3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "458def3d-487e-447a-9bf2-a56ecedb12cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Job_ratings\n",
      "mean     3.730213\n",
      "592\n"
     ]
    }
   ],
   "source": [
    "# The average indeed ratings of Institutions that listed the jobs\n",
    "print(df_updated.agg({\"Job_ratings\": [\"mean\"]}))\n",
    "print(len(df_updated.Company.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "452ebc67-2f7c-4da1-8680-758b6191cb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results of analysis in csv\n",
    "df_updated.to_csv('JobDetailsAnalysis.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
